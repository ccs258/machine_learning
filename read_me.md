# 线性回归模型的学习

## 理解线性回归的模型
   
   包括模型的目标函数,模型的损失函数;
   
   
## 针对目标函数进行求解
   * ### 理解梯度下降法：理解梯度的概念(本身是指函数变化最快的方向)，梯度下降算法(一般梯度和随机梯度)，理解凸优化函数的前提(要么有极大要么有极小)，
   
   * ### 最小二乘法：用矩阵向量表达形式，直接数学求解，缺陷：有可能矩阵不可逆，不能求；
   
   * ### 牛顿法：使用函数的泰勒技术前面几项来寻找f（x）= 0的根;首先用太累泰勒展开到二阶导，这个地方注意重复指的是对参数theta的迭代，而不是对样本值的迭代，所以刚开始只需要初始化一个参数theta，另外，又由于凸函数的性质，有且只有一个极小值（其实是零值点，因为这个地方做的是对损失函数区域0值的泰勒展开），该性质决定了其二阶导永远大于0，一阶导可正可负，但当一阶导为正时，当前Xk,经迭代公式计算后，Xk+1会移到Xk的左边；当一阶导为负的时候，经迭代公式计算后，Xk+1会移到Xk的左边；她们都是在往函数取0点的最低点方向移动！，此处的Xk指的是模型参数theta;即theta逐步迭代，使得函数越来越趋近于取最小值0值；需要注意的是，此处一阶导、二阶导是针对x(此处的x是自变量x)的导数，当x为一元变量的时候，就是常见的一阶导和二阶导，但当x为二元甚至多元变量的时候，就是多元一阶导，跟多元二阶导(也即海森矩阵)，而theta只是f(x)模型的参数，
   
   * ### 拟牛顿法：牛顿法虽然计算快，但需要计算多元二阶导(注意，重点是体现在多元，而不是二阶导，针对每两个变量交叉)（海森矩阵)的逆矩阵，而且目标函数的海森矩阵可能无法保证正定，牛顿法失效，提出了拟牛顿法，思路是：不用二阶偏导数而构造出可以近似海森矩阵的正定对称阵。不同的构造方法就产生了不同的拟牛顿法；
   
   
# 贝叶斯模型的学习

## 理解生成模型和判别模型
   
   先验概率和后验概率，以及理解生成模型的理论以及编码实现demo;
   
    Task02：朴素贝叶斯（2天）
    理论部分
    
    相关概念
    生成模型
    判别模型
    朴素贝叶斯基本原理
    条件概率公式
    乘法公式
    全概率公式
    贝叶斯定理
    特征条件独立假设
    后验概率最大化
    拉普拉斯平滑
    朴素贝叶斯的三种形式
    高斯型
    多项式型
    伯努利型
    极值问题情况下的每个类的分类概率
    下溢问题如何解决
    零概率问题如何解决
    sklearn参数详解
    练习部分
    
    利用sklearn解决聚类问题。
    sklearn.naive_bayes.GaussianNB
    
    
    ###
    朴素贝叶斯的三种模型
    
   ###朴素贝叶斯理论推导与三种常见模型
   
    总体思想：不同类别对应该已知条件的后验概率，比较最大的那个来决定其类别预测值；

    多项式、伯努利、高斯
      
    多项式：针对离散特征，可考虑平滑处理，前提是认为特征与特征之间独立，通过求已有样本中，
   
    高斯：针对的是连续型样本，可以通过计算不同类别下特征服从的分布（mu,sigma）,来进行该连续取值得相对密度函数概率值的计算，再利用上述的思想
    
    伯努利：同多项式，也属于离散型特征，不同的是伯努利是服从0-1分布的，因此其概率计算略有不同
    
    参考：https://blog.csdn.net/u012162613/article/details/48323777
    
    
   ###后验概率最大化
   让后验概率最大化，损失函数才会最小，样本i预测为其他类别的风险求和总体风险；
   ###拉普拉斯平滑

   防止太小（连续相乘会特别小）或没有出现过为0值；
   拉普拉斯平滑的原因是:防止下溢，某些类别其特征在训练集没有出现过，但是实际中可能会出现，训练集不可能包含所有的情况，防止没有直接归为0，
   做拉普拉斯的平滑处理；
   
   对于先验概率和条件概率的平滑；
   平滑之后仍然满足概率和为1；
   
      参考：
      贝叶斯网络、拉普拉斯平滑
      
        如果仅仅是区分垃圾邮件，则不需要算p(x)，因为对于公式：
              P(c1 | x) = ( P(x | c1) * P(c1) ) / P(x)
              P(c2 | x) = ( P(x | c2) * P(c2) ) / P(x)
        两个的p(x) 是一样的，既然是一样的那对于区分一个邮件是否是垃圾邮件就没有帮助了。
        PS2，两个极端情况：
            1，很多次出现的次数都很小，这样的话P(x)因为是那么多很小的数相乘，则早就溢出了。这时候使用对P(x)取对数的方法解决，比如10-18，这个数放计算机早溢出了，但取了对数后是ln10-18=-18，就不可能溢出了。
            2，如果某个词没出现的话，那p(xi)就是0了，解决方法是拉普拉斯平滑。
        ————————————————
        版权声明：本文为CSDN博主「血影雪梦」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
        原文链接：https://blog.csdn.net/xueyingxue001/java/article/details/52396170
        
        
##EM算法的概率论基础概念的理解：
    试验定义：获得一次观测或测量的过程；
    假设有3枚硬币，分别记做A、B、C，这些硬币正面出现的概率分别是π、p、q，进行如下实验
    step1:先掷硬币A，根据结果选出硬币B和硬币C，正面选硬币B，反面选硬币C
    step2:通过选择出的硬币，掷硬币的结果出现正面为1，反面为0
    如此独立地重复n次实验，我们当前规定n=10，则10次的结果如下所示：1,1,0,1,0,0,1,0,1,1
    
    本案例中的试验可以这样定义：先掷硬币A,选出硬币B和硬币C,再根据选择出的硬币投掷出现正面的过程

    简单事件定义：是一个试验的基本结果，它不能分解为更简单的结果。
    基于试验定义，本试验中的简单事件可以定义为： （1）投掷硬币A选出B，投掷B为正面；
                                            （2）投掷硬币A选出C，投掷C为正面；
    
    事件定义：是简单事件的一个指定集合。
    本案例中的事件是：先掷硬币A,选出硬币B和硬币C,再根据选择出的硬币投掷出现正面，此事件是上述简单事件的集合； 
    
    事件的概率：事件A的概率等于事件A中所包含的简单事件的概率之和
    因此，本试验中事件的概率为：P(1∣θ)=πp+(1−π)q   

##高斯混合分布的理解
     背景：常见的产生数据，来自单一的高斯分布，根据这些数据求最佳的参数（均值，方差）很容易，直接求导令其等于0；
     但实际中往往有些数据的分布是多个堆的，因此一个高斯分布不合理，需要多个高斯分布刻画，那针对某一个数据点的概率分布来说，多个高斯分布
     的概率分布就是多个概率分布求和，针对这部分求导可以，但令其等于0求解比较困难，因此需要引入迭代法求解。考虑到。可以
     引入辅助变量（也叫隐变量），刻画当前数据是来自于哪个高斯分布，对于辅助变量来说，其分布就是有多个点对应当前的第k个高斯
     分布，引入辅助变量后，对其最大化，由贝叶斯概率公式，以及theta的参数以及隐变量的式子求关于隐变量的积分，根据函数的
     期望与期望的函数的关系(convex关系，函数的期望是线性组合，期望的函数则是函数本身的值)得到其收敛；另外对于一个场景：
     求一个含x,y的参数的最优值，并且其约束含x,y参数，最好的方式是用拉格朗日公式，移项，分别求导；
     
     目标函数：argmaxL(mu,sigma,Z)
     迭代公式：
     总共有三组参数(均值，方差，alpha（自由度k-1,因为共k个，隐变量和为1，因此只需要知道k-1个，就可以知道第k个）)
     E步：
     alpha的迭代；不同高斯分布的权重迭代；每一次迭代所有样本点都共用的一组参数(一维)；元素个数(列数)与跟高斯分布个数一致；
     其中，影响隐变量的迭代，每一次迭代每个样本点各自不同的参数；元素个数与跟样本个数(行数)及高斯分布(列数)相关；
     隐变量：其实是指混合高斯分布中的每个高斯分布的权重，对于一个数据而言，其隐变量值权重，分母等于所有在该点处的高斯概率分布取值之和，
     分子等于当前隐变量所隶属的堆(即高斯)的高斯值；每一个数据点都会有一组隐变量值，反映当前这个数据点在不同高斯分布的隶属情况；
     第K个高斯分布的均值：所有隶属于当前高斯分布的数值取值的均值；（当你有了数据以后，这个数据属于第k个高斯的概率），没有那些概率，
     它就是单个高斯分布概率均值的算法；每一次迭代所有样本点都共用的一组参数(一维)；元素个数(列数)与跟高斯分布个数一致；
     第K个高斯分布的方差：每一次迭代所有样本点都共用的一组参数(一维)；元素个数(列数)与跟高斯分布个数一致；
     
     trace:矩阵的主对角线的元素加起来；XX^T=是矩阵，X^TX是一维；高斯高维的情况下就会用到X^TX；
     
     多元高斯分布；
       