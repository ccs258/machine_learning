# 线性回归模型的学习

## 理解线性回归的模型
   
   包括模型的目标函数,模型的损失函数;
   
   
## 针对目标函数进行求解
   * ### 理解梯度下降法：理解梯度的概念(本身是指函数变化最快的方向)，梯度下降算法(一般梯度和随机梯度)，理解凸优化函数的前提(要么有极大要么有极小)，
   
   * ### 最小二乘法：用矩阵向量表达形式，直接数学求解，缺陷：有可能矩阵不可逆，不能求；
   
   ###牛顿法：使用函数的泰勒技术前面几项来寻找f（x）= 0的根;首先用太累泰勒展开到二阶导，这个地方注意重复指的是对参数theta的迭代，而不是对样本值的迭代，所以刚开始只需要初始化一个参数theta，另外，又由于凸函数的性质，有且只有一个极小值（其实是零值点，因为这个地方做的是对损失函数区域0值的泰勒展开），该性质决定了其二阶导永远大于0，一阶导可正可负，但当一阶导为正时，当前Xk,经迭代公式计算后，Xk+1会移到Xk的左边；当一阶导为负的时候，经迭代公式计算后，Xk+1会移到Xk的左边；她们都是在往函数取0点的最低点方向移动！，此处的Xk指的是模型参数theta;即theta逐步迭代，使得函数越来越趋近于取最小值0值；
   需要注意的是，此处一阶导、二阶导是针对x(此处的x是自变量x)的导数，当x为一元变量的时候，就是常见的一阶导和二阶导，但当x为二元甚至多元变量的时候，就是多元一阶导，跟多元二阶导(也即海森矩阵)，而theta只是f(x)模型的参数，
   
   * ### 拟牛顿法：牛顿法虽然计算快，但需要计算多元二阶导(注意，重点是体现在多元，而不是二阶导，针对每两个变量交叉)（海森矩阵)的逆矩阵，而且目标函数的海森矩阵可能无法保证正定，牛顿法失效，提出了拟牛顿法，思路是：不用二阶偏导数而构造出可以近似海森矩阵的正定对称阵。不同的构造方法就产生了不同的拟牛顿法；
   
   
