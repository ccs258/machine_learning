# (1)线性回归模型的学习

## 理解线性回归的模型
   
    包括模型的目标函数,模型的损失函数;
   
   
## 针对目标函数进行求解
       * 理解梯度下降法：理解梯度的概念(本身是指函数变化最快的方向)，梯度下降算法(一般梯度和随机梯度)，理解凸优化函数的前提(要么有极大要么有极小)，
       
       * 最小二乘法：用矩阵向量表达形式，直接数学求解，缺陷：有可能矩阵不可逆，不能求；
       
       * 牛顿法：使用函数的泰勒技术前面几项来寻找f（x）= 0的根;首先用太累泰勒展开到二阶导，这个地方注意重复指的是对参数theta的迭代，而不是对样本值的迭代，所以刚开始只需要初始化一个参数theta，另外，又由于凸函数的性质，有且只有一个极小值（其实是零值点，因为这个地方做的是对损失函数区域0值的泰勒展开），该性质决定了其二阶导永远大于0，一阶导可正可负，但当一阶导为正时，当前Xk,经迭代公式计算后，Xk+1会移到Xk的左边；当一阶导为负的时候，经迭代公式计算后，Xk+1会移到Xk的左边；她们都是在往函数取0点的最低点方向移动！，此处的Xk指的是模型参数theta;即theta逐步迭代，使得函数越来越趋近于取最小值0值；需要注意的是，此处一阶导、二阶导是针对x(此处的x是自变量x)的导数，当x为一元变量的时候，就是常见的一阶导和二阶导，但当x为二元甚至多元变量的时候，就是多元一阶导，跟多元二阶导(也即海森矩阵)，而theta只是f(x)模型的参数，
       
       * 拟牛顿法：牛顿法虽然计算快，但需要计算多元二阶导(注意，重点是体现在多元，而不是二阶导，针对每两个变量交叉)（海森矩阵)的逆矩阵，而且目标函数的海森矩阵可能无法保证正定，牛顿法失效，提出了拟牛顿法，思路是：不用二阶偏导数而构造出可以近似海森矩阵的正定对称阵。不同的构造方法就产生了不同的拟牛顿法；
       
   
# （2）贝叶斯模型的学习

## 理解生成模型和判别模型
   
    先验概率和后验概率，以及理解生成模型的理论以及编码实现demo;
      
   ###朴素贝叶斯理论推导与三种常见模型
   
   
    总体思想：不同类别对应该已知条件的后验概率，比较最大的那个来决定其类别预测值；

    多项式、伯努利、高斯
      
    多项式：针对离散特征，可考虑平滑处理，前提是认为特征与特征之间独立，通过求已有样本中，
   
    高斯：针对的是连续型样本，可以通过计算不同类别下特征服从的分布（mu,sigma）,来进行该连续取值得相对密度函数概率值的计算，再利用上述的思想
    
    伯努利：同多项式，也属于离散型特征，不同的是伯努利是服从0-1分布的，因此其概率计算略有不同
    
    参考：https://blog.csdn.net/u012162613/article/details/48323777
    
    
   ### 后验概率最大化
   
    让后验概率最大化，损失函数才会最小，样本i预测为其他类别的风险求和总体风险；
   
   ### 拉普拉斯平滑

       防止太小（连续相乘会特别小）或没有出现过为0值；
       拉普拉斯平滑的原因是:防止下溢，某些类别其特征在训练集没有出现过，但是实际中可能会出现，训练集不可能包含所有的情况，防止没有直接归为0，
       做拉普拉斯的平滑处理；
       
       对于先验概率和条件概率的平滑；
       平滑之后仍然满足概率和为1；
   
      参考：
      贝叶斯网络、拉普拉斯平滑
      
        如果仅仅是区分垃圾邮件，则不需要算p(x)，因为对于公式：
              P(c1 | x) = ( P(x | c1) * P(c1) ) / P(x)
              P(c2 | x) = ( P(x | c2) * P(c2) ) / P(x)
        两个的p(x) 是一样的，既然是一样的那对于区分一个邮件是否是垃圾邮件就没有帮助了。
        PS2，两个极端情况：
            1，很多次出现的次数都很小，这样的话P(x)因为是那么多很小的数相乘，则早就溢出了。这时候使用对P(x)取对数的方法解决，比如10-18，这个数放计算机早溢出了，但取了对数后是ln10-18=-18，就不可能溢出了。
            2，如果某个词没出现的话，那p(xi)就是0了，解决方法是拉普拉斯平滑。
        ————————————————
        版权声明：本文为CSDN博主「血影雪梦」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
        原文链接：https://blog.csdn.net/xueyingxue001/java/article/details/52396170
        
        
        
# （3）EM算法

## 统计学试验事件基础概念的理解
    试验定义：获得一次观测或测量的过程；
    假设有3枚硬币，分别记做A、B、C，这些硬币正面出现的概率分别是π、p、q，进行如下实验
    step1:先掷硬币A，根据结果选出硬币B和硬币C，正面选硬币B，反面选硬币C
    step2:通过选择出的硬币，掷硬币的结果出现正面为1，反面为0
    如此独立地重复n次实验，我们当前规定n=10，则10次的结果如下所示：1,1,0,1,0,0,1,0,1,1
    
    本案例中的试验可以这样定义：先掷硬币A,选出硬币B和硬币C,再根据选择出的硬币投掷出现正面的过程

    简单事件定义：是一个试验的基本结果，它不能分解为更简单的结果。
    基于试验定义，本试验中的简单事件可以定义为： （1）投掷硬币A选出B，投掷B为正面；
                                            （2）投掷硬币A选出C，投掷C为正面；
    
    事件定义：是简单事件的一个指定集合。
    本案例中的事件是：先掷硬币A,选出硬币B和硬币C,再根据选择出的硬币投掷出现正面，此事件是上述简单事件的集合； 
    
    事件的概率：事件A的概率等于事件A中所包含的简单事件的概率之和
    因此，本试验中事件的概率为：P(1∣θ)=πp+(1−π)q   


## 高斯混合分布的理解

     背景：常见的产生数据，来自单一的高斯分布，根据这些数据求最佳的参数（均值，方差）很容易，直接求导令其等于0；
     但实际中往往有些数据的分布是多个堆的，因此一个高斯分布不合理，需要多个高斯分布刻画，那针对某一个数据点的概率分布来说，多个高斯分布
     的概率分布就是多个概率分布求和，针对这部分求导可以，但令其等于0求解比较困难，因此需要引入迭代法求解。考虑到。可以
     引入辅助变量（也叫隐变量），刻画当前数据是来自于哪个高斯分布，对于辅助变量来说，其分布就是有多个点对应当前的第k个高斯
     分布，引入辅助变量后，对其最大化，由贝叶斯概率公式，以及theta的参数以及隐变量的式子求关于隐变量的积分，根据函数的
     期望与期望的函数的关系(convex关系，函数的期望是线性组合，期望的函数则是函数本身的值)得到其收敛；另外对于一个场景：
     求一个含x,y的参数的最优值，并且其约束含x,y参数，最好的方式是用拉格朗日公式，移项，分别求导；
     
     目标函数：argmaxL(mu,sigma,Z)
     迭代公式：
     总共有三组参数(均值，方差，alpha（自由度k-1,因为共k个，隐变量和为1，因此只需要知道k-1个，就可以知道第k个）)
     E步：
     alpha的迭代；不同高斯分布的权重迭代；每一次迭代所有样本点都共用的一组参数(一维)；元素个数(列数)与跟高斯分布个数一致；
     其中，影响隐变量的迭代，每一次迭代每个样本点各自不同的参数；元素个数与跟样本个数(行数)及高斯分布(列数)相关；
     隐变量：其实是指混合高斯分布中的每个高斯分布的权重，对于一个数据而言，其隐变量值权重，分母等于所有在该点处的高斯概率分布取值之和，
     分子等于当前隐变量所隶属的堆(即高斯)的高斯值；每一个数据点都会有一组隐变量值，反映当前这个数据点在不同高斯分布的隶属情况；
     第K个高斯分布的均值：所有隶属于当前高斯分布的数值取值的均值；（当你有了数据以后，这个数据属于第k个高斯的概率），没有那些概率，
     它就是单个高斯分布概率均值的算法；每一次迭代所有样本点都共用的一组参数(一维)；元素个数(列数)与跟高斯分布个数一致；
     第K个高斯分布的方差：每一次迭代所有样本点都共用的一组参数(一维)；元素个数(列数)与跟高斯分布个数一致；
     
     trace:矩阵的主对角线的元素加起来；XX^T=是矩阵，X^TX是一维；高斯高维的情况下就会用到X^TX；
     
     多元高斯分布的扩展；
     
### 4、 马尔科夫
##（1）马尔科夫过程
        定义
        1. 马尔科夫假设
        应该是齐次马尔科夫假设，这样假设：马尔科夫链 （x_{1},\cdots,x_{n}) 里的 x_{i} 总是只受 x_{i-1} 一个人的影响。
        马尔科夫假设这里相当于就是个1-gram。
        
        马尔科夫过程呢？即，在一个过程中，每个状态的转移只依赖于前n个状态，并且只是个n阶的模型。最简单的马尔科夫过程是一阶的，即只依赖于器哪一个状态。

## （2） 隐马尔科夫过程
       定义
       正所谓隐含马尔科夫模型，那么除了正常的节点，还要将隐含状态节点也得建模进去。正儿八经地，将 X_{i} 、 Y_{i} 换成 i_{i} 、o_{i} ,并且他们的名称变为状态节点、观测节点。状态节点正是我的隐状态。
       
       包含的数据：
       观测节点、隐藏节点、观测状态转移矩阵(这个就是隐藏节点的转移矩阵)、观测矩阵；
       
       数据建模：
       已知观测节点Ot,要预测Ot+1;
       根据Ot得到对应的Bt(隐藏变量)，意味着观测变量依赖于隐藏变量；
       根据Bt得到Bt+1;再根据Bt+1得到Ot+1，意味着隐藏变量满足一阶马尔可夫性。
       本质是一个生成式模型；
       
       对应博客中，高层次的理解：
        根据概率图分类，可以看到HMM属于有向图，并且是生成式模型，直接对联合概率分布建模 P(O,I) = \sum_{t=1}^{T}P(O_{t} | O_{t-1})P(I_{t} | O_{t}) (注意，这个公式不在模型运行的任何阶段能体现出来，只是我们都去这么来表示HMM是个生成式模型，他的联合概率 P(O,I) 就是这么计算的)。
        并且B中 b_{ij} = P(o_{t}|i_{t}) ，这意味着o对i有依赖性。
        在A中， a_{ij} = P(i_{t+1}|i_{t}) ，也就是说只遵循了一阶马尔科夫假设，1-gram。试想，如果数据的依赖超过1-gram，那肯定HMM肯定是考虑不进去的。这一点限制了HMM的性能
       
## （3）条件随机场
        定义
        计算问题
        
        预测问题
        
## （4）Baum-Welch算法
        
        
        （1）熟悉隐马尔可夫模型定义
        
        隐马尔可夫模型描述了一组“隐含”变量和可观测到的离散随机变量的联合概率。它依赖于假设：第i个隐藏变量只与第i-1个隐含变量相关，而与其他先前的隐藏变量无关，而当前观测到的状态仅依赖于当前的隐藏状态。
        Baum-Welch算法利用EM算法，在给定一组观测特征向量的情况下，求出隐马尔可夫模型参数的最大似然估计。
        
        （2）隐马尔科夫的数学表示过程
        观测变量，假设有K种取值，也即观测序列；
        隐藏变量，假设有N种状态；
        t与t+1时刻隐藏变量的关系，t+1时刻的隐藏变量只与t时刻隐藏变量相关，由此可得隐藏变量转移矩阵A；
        t时刻观测变量只与t时刻隐藏变量相关；由此可得到观测变量转移矩阵；矩阵大小为：N*K,记作B；
        
        上述过程可以用theta(A,B,pi)描述马尔可夫链，Baum-Welch算法寻找argmaxP(Y|theta)的局部极大值，也就是能够使得观测到的序列出现的概率最大的HMM的参数theta; 

        （3）算法
        初始化theta(A,B,pi)，可以随机初始化，或者根据先验知识初始化。
        前向过程
        ai(t) = P(Y1=y1,Y2=y2,...,Yt=yt,Xt=i|theta)
        实在给定参数theta的条件下，观测序列是y1,y2,...yt,时刻t的状态是i的概率；
        
        后向过程
        bi(t) = P(Yt+1 = yt+1,...,Yt=yt|Xt=i,theta)是参数是theta,在时刻t的状态是i的条件下，余下部分的观测序列是yt+1,..,yT的概率。
        可以看出，
        
        更新

### 5 逻辑回归

        参考维基百科：https://zh.wikipedia.org/wiki/%E9%82%8F%E8%BC%AF%E8%BF%B4%E6%AD%B8
        
        背景信息（到底什么是逻辑回归？）：
        普通回归处理寻找一种函数，该函数将 连续结果变量（因变量y）与一个或多个预测变量（因变量x 1，x 2等）相关。简单线性回归假定函数为以下形式：
        y = c 0 + c 1 * x 1 + c 2 * x 2 + ...，
        并找到c 0，c 1，c 2等值（c 0为称为“拦截”或“常数项”）。
        
        Logistic回归是普通回归的一种变体，当观察到的结果限制为两个值（通常代表某个结果事件的发生或不发生）（通常分别编码为1或0）时很有用。它产生一个公式，该公式根据自变量来预测发生的可能性。
        
        通过进行线性回归（上述），逻辑回归拟合特殊的S形曲线，该线性回归可以产生负无穷大和正无穷大之间的任何y值，并使用以下函数对其进行变换：
        p = Exp（y）/（1 + Exp （y））
        ，其p值介于0（y接近负无穷大）和1（y接近正无穷大）之间。现在，这成为一种特殊的非线性回归，这就是该页面执行的操作。
        
        Logistic回归还会产生与每个预测变量值相关的赔率（OR）。事件的几率定义为结果事件发生的概率除以事件未发生的概率 。预测变量的比值比表示当预测变量值的值增加1.0个单位时结果的几率增加（或大于1.0）或减少（或小于1.0）的相对量。




## 参考

https://blog.csdn.net/Scythe666/article/details/82021692

https://zhuanlan.zhihu.com/p/108620868

https://blog.csdn.net/fengyanqingnudt/article/details/84135997


Baum-Welch算法经典参考维基百科；https://zh.wikipedia.org/wiki/Baum-Welch%E7%AE%97%E6%B3%95
   
   
##拓展
       本质思维培养有很多方法， 这里参考前人经验，介绍两种最基本的方式。

        1. “学习-实践-复盘-优化”来沉淀自己的本质思维
        你要先学习前人的经验，不管有没有真正理解，但先搭起你对这个领域认识的一个基本框架。
        
        然后再拿你所学到的这些东西去实践应用，在实际的成功和失败中获取更深的体验和更透彻的理解。
        
        不管成功还失败，只有自己主动复盘，才能彻底的将你的实践价值发挥到最大化，也只有复盘，才是你从一件事情中获得经验和新知的最佳途径。
        
        那有了新经验、新知识之后，你就要将他们与你的旧理论进行整合优化。
        
        然后再不断的学习，不断的实践，重复的进行这些步骤，直至你的看待事物本质的能力越来越趋于成熟。
        
        这是一套强调实践的方法。
        
        2. 定义-类比-比方法
        这是《直击本质》书中提供的方法，有三种方式可参考：给出清晰的定义、做出准确的简单类比、打出精妙的比方。
        
        （1）给出清晰的定义
        
        定义是揭示概念所反映对象的特点或本质的一种逻辑方法。清晰准确的定义，本身说明你对事物已有深入认知。
        
        刘润老师给零售下的定义，就是把最终付钱的“人”（消费者）和“货”（商品）连接在一起的“场”。不管线下菜市场、便利店，还是线上电商、直播卖货，都一个连接“人”和“货”的“场”。
        
        （2）做出准确的简单类比
        
        如果能用类比，把一件事说成大白话，让别人都能明白，那一定是接近事物本质了。
        
        亚里士多德类比过“人”，说“人是理性的动物 ”。说人是动物，但又有别于一般的动物，是有理性思维的。这也成为经典的关于人的定义。
        
        （3）打出精妙的比方
        
        能够打出精妙的比方就意味着，我们能将一个抽象事物的根本属性与生活、工作中常见事物的根本属性画上等号。
        
        《人类简史》的作者赫拉利打过一个精妙的比方，他说，恐怖分子就像是一只想摧毁瓷器店的苍蝇，但它自身没那么大的力量，于是它就钻进公牛的耳朵里，让公牛发疯，然后冲进瓷器店。
        
        在这个比方中，赫拉利将恐怖分子比作“想摧毁瓷器店的苍蝇”，这一比方十分精妙。而其精妙的原因就在于，它切中了恐怖分子的根本属性：单薄的自身能力，以及庞大的目标。
        
        最后，区分我们通过分析得到的观点是否是“本质”，一个判断标准是：这个观点是在描述现象，还是根本原因。
        
        另一个标准是，你分析的结论是一个还是多个，如果是多个，说明还没有触及本质。本质只有一个   
        
        
        
        为什么你的月薪一直突破不了两万？
        没有无缘无故的高薪，在残酷却公平的职场里，高薪的背后，必然是高出普通员工的个人价值，是一个人在职场中的日益重要，是大气的格局。

        当你大气、聪明、勤奋、对自己足够苛刻、把握当下看到全局，你就会拥有你意想不到的月薪。这样的职场常识，愿你能够尽早领悟。 