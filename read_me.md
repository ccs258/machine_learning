# 线性回归模型的学习

## 理解线性回归的模型
   
   包括模型的目标函数,模型的损失函数;
   
   
## 针对目标函数进行求解
   * ### 理解梯度下降法：理解梯度的概念(本身是指函数变化最快的方向)，梯度下降算法(一般梯度和随机梯度)，理解凸优化函数的前提(要么有极大要么有极小)，
   
   * ### 最小二乘法：用矩阵向量表达形式，直接数学求解，缺陷：有可能矩阵不可逆，不能求；
   
   * ### 牛顿法：使用函数的泰勒技术前面几项来寻找f（x）= 0的根;首先用太累泰勒展开到二阶导，这个地方注意重复指的是对参数theta的迭代，而不是对样本值的迭代，所以刚开始只需要初始化一个参数theta，另外，又由于凸函数的性质，有且只有一个极小值（其实是零值点，因为这个地方做的是对损失函数区域0值的泰勒展开），该性质决定了其二阶导永远大于0，一阶导可正可负，但当一阶导为正时，当前Xk,经迭代公式计算后，Xk+1会移到Xk的左边；当一阶导为负的时候，经迭代公式计算后，Xk+1会移到Xk的左边；她们都是在往函数取0点的最低点方向移动！，此处的Xk指的是模型参数theta;即theta逐步迭代，使得函数越来越趋近于取最小值0值；需要注意的是，此处一阶导、二阶导是针对x(此处的x是自变量x)的导数，当x为一元变量的时候，就是常见的一阶导和二阶导，但当x为二元甚至多元变量的时候，就是多元一阶导，跟多元二阶导(也即海森矩阵)，而theta只是f(x)模型的参数，
   
   * ### 拟牛顿法：牛顿法虽然计算快，但需要计算多元二阶导(注意，重点是体现在多元，而不是二阶导，针对每两个变量交叉)（海森矩阵)的逆矩阵，而且目标函数的海森矩阵可能无法保证正定，牛顿法失效，提出了拟牛顿法，思路是：不用二阶偏导数而构造出可以近似海森矩阵的正定对称阵。不同的构造方法就产生了不同的拟牛顿法；
   
   
# 贝叶斯模型的学习

## 理解生成模型和判别模型
   
   先验概率和后验概率，以及理解生成模型的理论以及编码实现demo;
   
    Task02：朴素贝叶斯（2天）
    理论部分
    
    相关概念
    生成模型
    判别模型
    朴素贝叶斯基本原理
    条件概率公式
    乘法公式
    全概率公式
    贝叶斯定理
    特征条件独立假设
    后验概率最大化
    拉普拉斯平滑
    朴素贝叶斯的三种形式
    高斯型
    多项式型
    伯努利型
    极值问题情况下的每个类的分类概率
    下溢问题如何解决
    零概率问题如何解决
    sklearn参数详解
    练习部分
    
    利用sklearn解决聚类问题。
    sklearn.naive_bayes.GaussianNB
    
    
    ###
    朴素贝叶斯的三种模型
    
   ###朴素贝叶斯理论推导与三种常见模型
   
    总体思想：不同类别对应该已知条件的后验概率，比较最大的那个来决定其类别预测值；

    多项式、伯努利、高斯
      
    多项式：针对离散特征，可考虑平滑处理，前提是认为特征与特征之间独立，通过求已有样本中，
   
    高斯：针对的是连续型样本，可以通过计算不同类别下特征服从的分布（mu,sigma）,来进行该连续取值得相对密度函数概率值的计算，再利用上述的思想
    
    伯努利：同多项式，也属于离散型特征，不同的是伯努利是服从0-1分布的，因此其概率计算略有不同
    
    参考：https://blog.csdn.net/u012162613/article/details/48323777